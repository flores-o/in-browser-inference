**Repository: ONNX Runtime Web Inference**

**Running the demo:**

```
As a temporary workaround to get the model weights

# run the following from root 
# git checkout gpt2-onnx
cd models/Xenova
git clone https://huggingface.co/Xenova/gpt2
```

``` 
npm install -g http-server
http-server -p 8081 --cors
```



