<!DOCTYPE html>
<html>
<head>
  <title>ONNX Runtime Web Inference - Performance Measurement</title>
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
  <script type="module">
    import { AutoTokenizer } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.16.0';

    async function runInferenceBenchmark() {
      const { session, tokenizer } = await initializeModelAndTokenizer();
      const texts = ["Your sample text here"]; // Replace with different input text
      const { input_ids, attention_mask } = prepareModelInputs(tokenizer, texts);
      const feeds = {
        input_ids: new ort.Tensor('int64', input_ids.data, [1, input_ids.data.length]),
        attention_mask: new ort.Tensor('int64', attention_mask.data, [1, attention_mask.data.length])
      };

      // Warm-up runs
      for (let i = 0; i < 10; i++) {
        await session.run(feeds);
      }

      // Benchmark runs
      const runTimes = [];
      for (let i = 0; i < 100; i++) {
        const startTime = performance.now();
        await session.run(feeds);
        const endTime = performance.now();
        runTimes.push(endTime - startTime);
      }

      // Calculate average time excluding warm-up
      // ONNX model inference time is in milliseconds 54.5 - 55.9 ms
      const averageTime = runTimes.reduce((a, b) => a + b, 0) / runTimes.length;
      console.log(`Average Inference Time (ONNX): ${averageTime.toFixed(2)} ms`);
    }

    async function initializeModelAndTokenizer() {
      // Model source https://huggingface.co/Xenova/gpt2/tree/main
      const session = await ort.InferenceSession.create('models/Xenova/gpt2/onnx/decoder_model.onnx');
      const tokenizer = await AutoTokenizer.from_pretrained('Xenova/gpt2');
      return { session, tokenizer };
    }

    function prepareModelInputs(tokenizer, texts) {
      tokenizer.padding_side = 'left';
      const { input_ids, attention_mask } = tokenizer(texts, {
        add_special_tokens: false,
        padding: true,
        truncation: true,
      });
      return { input_ids, attention_mask, totalInputTokens: input_ids.data.length };
    }

    // Run the benchmark
    runInferenceBenchmark();
  </script>
</head>
<body>
  <h1>ONNX Runtime Web Inference - Performance Measurement</h1>
</body>
</html>
