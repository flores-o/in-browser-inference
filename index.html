<!DOCTYPE html>
<html>
<head>
  <title>ONNX Runtime Web Inference - Performance Measurement</title>
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
  <script type="module">
    import { AutoTokenizer } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.16.0';

    async function runInferenceBenchmark() {
      const { session, tokenizer } = await initializeModelAndTokenizer();
      const texts = ['This is an example sentence', 'Each sentence is converted']
      const { input_ids, token_type_ids, attention_mask } = prepareModelInputs(tokenizer, texts);
      console.log("input_ids", input_ids.data);
      console.log("token_type_ids", token_type_ids.data);
      console.log("attention_mask", attention_mask.data);
      const feeds = {
        input_ids: new ort.Tensor('int64', input_ids.data, [1, input_ids.data.length]),
        token_type_ids: new ort.Tensor('int64', token_type_ids.data, [1, token_type_ids.data.length]),
        attention_mask: new ort.Tensor('int64', attention_mask.data, [1, attention_mask.data.length])
      };

      // Perform one inference for sanity check
      const result = await session.run(feeds);
      console.log(result);

      // const decodedResult = decodeResult(result, tokenizer); // Assuming a decodeResult function exists
      // console.log(`Sanity Check Decoded Result: ${decodedResult}`);

      // Warm-up runs
      for (let i = 0; i < 10; i++) {
        await session.run(feeds);
      }

      // Benchmark runs
      const runTimes = [];
      for (let i = 0; i < 100; i++) {
        const startTime = performance.now();
        await session.run(feeds);
        const endTime = performance.now();
        runTimes.push(endTime - startTime);
      }

      // Calculate average time excluding warm-up
      const averageTime = runTimes.reduce((a, b) => a + b, 0) / runTimes.length;
      console.log(`Average Inference Time (ONNX): ${averageTime.toFixed(2)} ms`);
    }

    async function initializeModelAndTokenizer() {
      // Model source https://huggingface.co/Xenova/all-MiniLM-L6-v2
      const session = await ort.InferenceSession.create('models/Xenova/all-MiniLM-L6-v2/onnx/model_fp16.onnx', { executionProviders: ['webgpu'], log_severity_level: 0 });
      const tokenizer = await AutoTokenizer.from_pretrained('Xenova/all-MiniLM-L6-v2');
      return { session, tokenizer };
    }

    function prepareModelInputs(tokenizer, texts) {

      const { input_ids, token_type_ids, attention_mask } = tokenizer(texts, {
        padding: true,
        truncation: true,
        return_tensors: 'pt'
      });
      return { input_ids, token_type_ids, attention_mask };
    }

    // Run the benchmark
    runInferenceBenchmark();
  </script>
</head>
<body>
  <h1>ONNX Runtime Web Inference - Performance Measurement</h1>
</body>
</html>
