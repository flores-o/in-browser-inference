<!DOCTYPE html>
<html>
<head>
  <title>ONNX Runtime Web Inference - Performance Measurement</title>
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
  <script type="module">
    import { AutoTokenizer } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.16.0';

    async function runInferenceBenchmark() {
      const { session, tokenizer } = await initializeModelAndTokenizer();
      const texts = ["Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense. Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so"];
      const { input_ids, token_type_ids, attention_mask } = prepareModelInputs(tokenizer, texts);
      console.log("input_ids", input_ids.data);
      console.log("token_type_ids", token_type_ids.data);
      console.log("attention_mask", attention_mask.data);
      const feeds = {
        input_ids: new ort.Tensor('int64', input_ids.data, [1, input_ids.data.length]),
        token_type_ids: new ort.Tensor('int64', token_type_ids.data, [1, token_type_ids.data.length]),
        attention_mask: new ort.Tensor('int64', attention_mask.data, [1, attention_mask.data.length])
      };

      // Perform one inference for sanity check
      const result = await session.run(feeds);
      console.log(result);

      // Warm-up runs
      for (let i = 0; i < 10; i++) {
        await session.run(feeds);
      }

      // Benchmark runs
      const runTimes = [];
      for (let i = 0; i < 100; i++) {
        const startTime = performance.now();
        await session.run(feeds);
        const endTime = performance.now();
        runTimes.push(endTime - startTime);
      }

      // Calculate average time excluding warm-up
      // For the input "Your sample text here." average time is around 19.8 - 19.89 ms
      const averageTime = runTimes.reduce((a, b) => a + b, 0) / runTimes.length;
      console.log(`Average Inference Time (ONNX): ${averageTime.toFixed(2)} ms`);
    }

    async function initializeModelAndTokenizer() {
      // Model source https://huggingface.co/Xenova/all-MiniLM-L6-v2
      const session = await ort.InferenceSession.create('models/Xenova/all-MiniLM-L6-v2/onnx/model_fp16.onnx');
      const tokenizer = await AutoTokenizer.from_pretrained('Xenova/all-MiniLM-L6-v2');
      return { session, tokenizer };
    }

    function prepareModelInputs(tokenizer, texts) {

      const { input_ids, token_type_ids, attention_mask } = tokenizer(texts, {
        padding: true,
        truncation: true,
        max_length: 128,
        return_tensors: 'pt'
      });
      return { input_ids, token_type_ids, attention_mask };
    }

    // Run the benchmark
    runInferenceBenchmark();
  </script>
</head>
<body>
  <h1>ONNX Runtime Web Inference - Performance Measurement</h1>
</body>
</html>
