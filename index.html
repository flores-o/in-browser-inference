<!DOCTYPE html>
<html>
<head>
  <title>TensorFlow JS Inference - Performance Measurement</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js"> </script>
  <!-- Add the WebGPU backend to the global backend registry -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgpu/dist/tf-backend-webgpu.js"></script>

  <script type="module">
    import { AutoTokenizer } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.16.0';

    async function runInferenceBenchmark() {
      console.log('Running Inference Benchmark');
      console.log('tf.getBackend()', tf.getBackend())

      const { model, tokenizer } = await initializeModelAndTokenizer();
      const texts = ["Your sample text here"]; // Replace with different input
      const { input_ids, attention_mask } = prepareModelInputs(tokenizer, texts);
      const inputIdsTensor = tf.tensor2d(Array.from(input_ids.data, bigint => Number(bigint)), [1, input_ids.data.length], 'int32');
      const attentionMaskTensor = tf.tensor2d(Array.from(attention_mask.data, bigint => Number(bigint)), [1, attention_mask.data.length], 'int32');

      // Perform one inference for sanity check
      const logitsTensor = await model.executeAsync({ 'input_ids': inputIdsTensor, 'attention_mask': attentionMaskTensor });
      const logits = logitsTensor.find(tensor => tensor.shape.length === 3); // Adjust based on your model's output
      const logitsArray = await logits.array();
      console.log(`Sanity Check Logits Shape: ${logits.shape}`);
      console.log(`Sanity Check Logits Array: ${logitsArray}`);
      const decodedResult = decodeResults(tokenizer, logitsArray);
      console.log(`Sanity Check Decoded Result: ${decodedResult.text}`);

      // Warm-up runs
      for (let i = 0; i < 10; i++) {
        await model.executeAsync({ 'input_ids': inputIdsTensor, 'attention_mask': attentionMaskTensor });
      }

      // Benchmark runs
      const runTimes = [];
      for (let i = 0; i < 100; i++) {
        const startTime = performance.now();
        await model.executeAsync({ 'input_ids': inputIdsTensor, 'attention_mask': attentionMaskTensor });
        const endTime = performance.now();
        runTimes.push(endTime - startTime);
      }

      // Calculate average time excluding warm-up
      // TFJS WebGPU model inference time is in milliseconds 16.10 - 16.99 ms
      const averageTime = runTimes.reduce((a, b) => a + b, 0) / runTimes.length;
      console.log(`Average Inference Time (TFJS): ${averageTime.toFixed(2)} ms`);
    }

    async function initializeModelAndTokenizer() {
      // Model pretrained GPT2 from HF Transformers library -> Tensorflow -> Tensorflow JS
      // See convert_to_tfjs.ipynb for conversion steps
      const model = await tf.loadGraphModel('./models/Xenova/test_gpt2_web_model/model.json');
      const tokenizer = await AutoTokenizer.from_pretrained('Xenova/gpt2');
      return { model, tokenizer };
    }

    function prepareModelInputs(tokenizer, texts) {
      tokenizer.padding_side = 'left';
      const { input_ids, attention_mask } = tokenizer(texts, {
        add_special_tokens: false,
        padding: true,
        truncation: true,
      });
      return { input_ids, attention_mask, totalInputTokens: input_ids.data.length };
    }

    // Helper function to apply softmax to logits
    function softmax(logits) {
      const maxLogit = Math.max(...logits);
      const scores = logits.map((l) => Math.exp(l - maxLogit));
      const sum = scores.reduce((a, b) => a + b, 0);
      return scores.map((s) => s / sum);
    }

    // Decode the output
    function decodeResults(tokenizer, logitsArray) {
      // logitsArray structure should match your model's output
      let tokenIds = [];
      for (let i = 0; i < logitsArray[0].length; i++) {
        const logitsSlice = logitsArray[0][i];
        const probabilities = softmax(logitsSlice);
        const maxProbIndex = probabilities.indexOf(Math.max(...probabilities));
        tokenIds.push(maxProbIndex);
      }
      const text = tokenizer.decode(tokenIds);
      return { decodedTokens: tokenIds.length, text };
    }

    // Run the benchmark
    // Set the backend to WebGPU and wait for the module to be ready
    tf.setBackend('webgpu').then(() => runInferenceBenchmark());
  </script>
</head>
<body>
  <h1>TensorFlow JS Inference - Performance Measurement</h1>
</body>
</html>

