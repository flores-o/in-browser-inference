<!DOCTYPE html>
<html>
<head>
  <title>ONNX Runtime Web Inference - Performance Measurement</title>
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
  <script type="module">
    import { AutoTokenizer } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.16.0';

    async function runInferenceBenchmark() {
      const { session, tokenizer } = await initializeModelAndTokenizer();
      const texts = ["That's what we do here. We've been doing it since May 2001. Which, by our reckoning, makes us the web's oldest daily writing project.Not sure where to begin? Try reading a few entries. Pick your favorite date or read a random entry. Or browse our writer profiles. Some of our members have been writing here nonstop for more than 10 years.Then, of course, when the next batch starts on the 1st of the month, click "]
      const { input_ids, token_type_ids, attention_mask } = prepareModelInputs(tokenizer, texts);
      console.log("input_ids", input_ids.data);
      console.log("token_type_ids", token_type_ids.data);
      console.log("attention_mask", attention_mask.data);
      const feeds = {
        input_ids: new ort.Tensor('int64', input_ids.data, [1, input_ids.data.length]),
        token_type_ids: new ort.Tensor('int64', token_type_ids.data, [1, token_type_ids.data.length]),
        attention_mask: new ort.Tensor('int64', attention_mask.data, [1, attention_mask.data.length])
      };

      // Perform one inference for sanity check
      const result = await session.run(feeds);
      console.log(result);

      // Warm-up runs
      for (let i = 0; i < 10; i++) {
        await session.run(feeds);
      }

      // Benchmark runs
      const runTimes = [];
      for (let i = 0; i < 100; i++) {
        const startTime = performance.now();
        await session.run(feeds);
        const endTime = performance.now();
        runTimes.push(endTime - startTime);
      }

      // Calculate average time excluding warm-up
      // Average time app 10 ms on an Apple M1 Pro, GPU 16 core
      const averageTime = runTimes.reduce((a, b) => a + b, 0) / runTimes.length;
      console.log(`Average Inference Time (ONNX): ${averageTime.toFixed(2)} ms`);
    }

    async function initializeModelAndTokenizer() {
      // Model source https://huggingface.co/Xenova/all-MiniLM-L6-v2
      const session = await ort.InferenceSession.create('models/Xenova/all-MiniLM-L6-v2/onnx/model_fp16.onnx', { executionProviders: ['webgpu'] });
      const tokenizer = await AutoTokenizer.from_pretrained('Xenova/all-MiniLM-L6-v2');
      return { session, tokenizer };
    }

    function prepareModelInputs(tokenizer, texts) {

      const { input_ids, token_type_ids, attention_mask } = tokenizer(texts, {
        padding: true,
        truncation: true,
        return_tensors: 'pt'
      });
      return { input_ids, token_type_ids, attention_mask };
    }

    // Run the benchmark
    runInferenceBenchmark();
  </script>
</head>
<body>
  <h1>ONNX Runtime Web Inference - Performance Measurement</h1>
</body>
</html>
