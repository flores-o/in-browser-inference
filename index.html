<!DOCTYPE html>
<html>
<head>
  <title>Tensorflow JS Inference</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
</head>
<body>
  <h1>Tensorflow JS Inference</h1>
  <label for="user-text">Enter text:</label>
  <input type="text" id="user-text" placeholder="Your text here" />
  <button id="generate-btn">Generate</button>
  <p>Generated text: <span id="generated-text-id"></span></p>
  <p>Stats: <span id="stats"></span></p>

  <script type="module">
    import { AutoTokenizer } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.16.0';

    async function initializeModelAndTokenizer() {
      const model = await tf.loadGraphModel('./models/Xenova/gpt2/tfjs_model/model.json');
      console.log('Model loaded');
      const tokenizer = await AutoTokenizer.from_pretrained('Xenova/gpt2');
      return { model, tokenizer };
    }

    function prepareModelInputs(tokenizer, texts) {
      tokenizer.padding_side = 'left';
      const { input_ids, attention_mask } = tokenizer(texts, {
        add_special_tokens: false,
        padding: true,
        truncation: true,
      });
      return { input_ids, attention_mask, totalInputTokens: input_ids.data.length };
    }

    function decodeResults(tokenizer, logitsTensor) {
      const sequenceLength = logitsTensor.shape[1];
      const logitsArray = logitsTensor.arraySync();
      let tokenIds = [];
      for (let i = 0; i < sequenceLength; i++) {
        let maxLogitIndex = logitsArray[0][i].indexOf(Math.max(...logitsArray[0][i]));
        tokenIds.push(maxLogitIndex);
      }
      const text = tokenizer.decode(tokenIds);
      return { decodedTokens: tokenIds.length, text };
    }

    async function runInference(texts) {
  try {
    const { model, tokenizer } = await initializeModelAndTokenizer();
    const { input_ids, attention_mask, totalInputTokens } = prepareModelInputs(tokenizer, texts);

    // Log the original BigInt64Array data
    console.log("Original input_ids.data:", input_ids.data);
    console.log("Original attention_mask.data:", attention_mask.data);

    // Convert BigInt64Array to an array of numbers with explicit conversion
    const inputIdsData = Array.from(input_ids.data, bigint => Number(bigint));
    const attentionMaskData = Array.from(attention_mask.data, bigint => Number(bigint));

    // Log the converted data for verification
    console.log("Converted input_ids.data:", inputIdsData);
    console.log("Converted attention_mask.data:", attentionMaskData);

    const inputIdsTensor = tf.tensor2d(inputIdsData, [1, inputIdsData.length], 'int32');
    const attentionMaskTensor = tf.tensor2d(attentionMaskData, [1, attentionMaskData.length], 'int32');

    console.log("inputIdsTensor shape:", inputIdsTensor.shape);
    console.log("attentionMaskTensor shape:", attentionMaskTensor.shape);

    const logitsTensor = model.predict({ input_ids: inputIdsTensor, attention_mask: attentionMaskTensor });

    const decodedText = decodeResults(tokenizer, logitsTensor);
    displayResults(decodedText.text);
  } catch (error) {
    console.error('Inference error:', error);
  }
}



    function displayResults(decodedText) {
      document.getElementById('generated-text-id').textContent = decodedText;
    }

    document.getElementById('generate-btn').addEventListener('click', () => {
      const userText = document.getElementById('user-text').value.trim();
      if (userText === "") {
        alert("Please enter some text.");
        return;
      }
      runInference(userText);
    });
  </script>
</body>
</html>
