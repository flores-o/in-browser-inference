<!DOCTYPE html>
<html>
<head>
  <title>TensorFlow JS Inference - Performance Measurement</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
  <script type="module">
    import { AutoTokenizer } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.16.0';

    async function runInferenceBenchmark() {
      const { model, tokenizer } = await initializeModelAndTokenizer();
      const texts = ["Your sample text here"]; // Replace with different input ?
      const { input_ids, attention_mask } = prepareModelInputs(tokenizer, texts);
      const inputIdsTensor = tf.tensor2d(Array.from(input_ids.data, bigint => Number(bigint)), [1, input_ids.data.length], 'int32');
      const attentionMaskTensor = tf.tensor2d(Array.from(attention_mask.data, bigint => Number(bigint)), [1, attention_mask.data.length], 'int32');

      // Warm-up runs
      for (let i = 0; i < 10; i++) {
        await model.executeAsync({ 'input_ids': inputIdsTensor, 'attention_mask': attentionMaskTensor });
      }

      // Benchmark runs
      const runTimes = [];
      for (let i = 0; i < 100; i++) {
        const startTime = performance.now();
        await model.executeAsync({ 'input_ids': inputIdsTensor, 'attention_mask': attentionMaskTensor });
        const endTime = performance.now();
        runTimes.push(endTime - startTime);
      }

      // Calculate average time excluding warm-up
      // TFJS model inference time is in milliseconds 76.5 - 77.9 ms
      const averageTime = runTimes.reduce((a, b) => a + b, 0) / runTimes.length;
      console.log(`Average Inference Time (TFJS): ${averageTime.toFixed(2)} ms`);
    }

    async function initializeModelAndTokenizer() {
      const model = await tf.loadGraphModel('./models/Xenova/test_gpt2_web_model/model.json');
      const tokenizer = await AutoTokenizer.from_pretrained('Xenova/gpt2');
      return { model, tokenizer };
    }

    function prepareModelInputs(tokenizer, texts) {
      tokenizer.padding_side = 'left';
      const { input_ids, attention_mask } = tokenizer(texts, {
        add_special_tokens: false,
        padding: true,
        truncation: true,
      });
      return { input_ids, attention_mask, totalInputTokens: input_ids.data.length };
    }

    // Run the benchmark
    runInferenceBenchmark();
  </script>
</head>
<body>
  <h1>TensorFlow JS Inference - Performance Measurement</h1>
</body>
</html>

